TODO 06.08.24:
2. Why is val accuracy messed up?
5. Adapt for other datasets — test
7. Fix the samples to plot - 100 random at beginning and then track the same samples throughout.(balanced between classes) ——- IMP -- done
8. Calculate cluster center with the plotted points. ——- IMP -- done
9. Fix radar plot like parallel - one sample goes through all features -- done
10. Fix parallel co ordinates dimensions axis plot -- done
11. Plot does not change with refresh -- 
12. Learning rate and batch size in cmd line arguments -- done
13. Debug the training.py - why 2356? Print shape of inputs and labels
14. Add other loss -- done, test
15. Fix plot interactivity(cluster movement) - IMP -- done
16. Improve performance of plotting - PCA and TSNE approach
17. make the alpha, beta, gamma modifiable during training -- done


Layer slection of model (with array), for training and validation/visualization --
pause after n epochs --
Fix the wrong datapoints color --
Force the rerender of radar and parallel if classes change --
selcting in scatter and highlight in radar and parallel plot --
check the feature importance -- while preparing the data

Put tool in executable
Tool Name
Save the loggings and the images of the plots, track the alpha beta...
Fix the axis tiles and legends, add the real label to legend --




Knowledge distillation loss(teacher - human interaction - interaction with cluster centers) try with higher dimensions and reduced dimensions
ensure that teacher and student have same features
keep fixed features for each epochs

alpha to balance with ce and kd loss

distance calculation -- ?  euclidean

teacher_logits - centers? batch? ?   use as teacher centers


knowledge distillation loss = teacher_logits and student_logits [distance between centers of teachers and students]


-----------------------------------------------------------------------------------------------------------------------


cross entropy vs our approach
check loss metrics to reduce[isolated experiments]
classic mse loss(teacher and student) yes
reduced dimensions[loss calculation] - mse loss and cross entropy with high dimensions





[cross entropy with high dim] vs [cross entropy with high dim and mse with low dimensions]
weight values in reference to the loss values

